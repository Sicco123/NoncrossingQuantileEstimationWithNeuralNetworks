import torch
import torch.nn as nn
from pytorch_NCMQRNN.l1_penalization_layer import l1_p

class GlobalDecoder(nn.Module):
    """
    Based on the hidden tensor generated by the Encoder and the values
    of the covariate time series in prediction horizon
    input_size = hidden_size + covariate_size * horizon_size
    output_size: (horizon_size+1) * context_size
    """
    def __init__(self,
                 hidden_size:int, 
                 covariate_size:int,
                 horizon_size:int,
                 context_size:int):
        super(GlobalDecoder,self).__init__()
        self.hidden_size = hidden_size
        self.covariate_size = covariate_size
        self.horizon_size = horizon_size
        self.context_size = context_size

        self.linear1 = nn.Linear(in_features= hidden_size ,
                                out_features= (horizon_size+1)*context_size)

        self.activation = nn.ReLU()
    def forward(self, input):
        layer1_output = self.linear1(input)
        layer1_output = self.activation(layer1_output)


        return layer1_output


class LocalDecoder(nn.Module):
    """
    Based on the resulting tensor generated by the GlobalDecoder and the 
    covariate time series value at prediction step.

    input_size: (horizon_size+1)*context_size + horizon_size*covariate_size
    output_size: horizon_size * quantile_size
    """
    def __init__(self,
                covariate_size, 
                quantile_size,
                context_size,
                quantiles,
                horizon_size):
        super(LocalDecoder,self).__init__()
        self.covariate_size = covariate_size
        self.quantiles = quantiles
        self.quantile_size = quantile_size
        self.horizon_size = horizon_size
        self.context_size = context_size

        self.linear1 = nn.Linear(in_features= (horizon_size+1)*context_size ,
                                 out_features= horizon_size* context_size)


        self.activation = nn.Sigmoid()
    
    def forward(self,input):
        layer1_output = self.linear1(input)
        layer1_output = self.activation(layer1_output)
        return layer1_output


class Penalizer(nn.Module):
    """
    Final layer of the decoder. Ensures the output to be noncrossing by using l1_p_layer components
    """
    def __init__(self,
                quantile_size,
                context_size,
                quantiles,
                horizon_size):
        super(Penalizer,self).__init__()

        self.quantiles = quantiles
        self.quantile_size = quantile_size
        self.horizon_size = horizon_size
        self.context_size = context_size

        self.l1_p_layers = nn.ModuleList()
        for h in range(horizon_size): # For every horizon we set a different layer
            l1_p_layer = l1_p(context_size*(horizon_size+1),
                             quantile_size)

            self.l1_p_layers.append(l1_p_layer)

    def forward(self, input):

        l1_penalties = torch.zeros((self.horizon_size))
        for h in range(self.horizon_size):
            l1_p_layer = self.l1_p_layers[h]

            for t in range(input.shape[0]):
                t_output, l1_penalty = l1_p_layer(input[t,:,:])
                h_output = t_output if t == 0 else torch.column_stack([h_output, t_output])
                l1_penalties[h] += l1_penalty

            quantile_output = h_output if h == 0 else torch.row_stack([quantile_output, h_output])

        quantile_output = quantile_output.view(input.shape[0], self.horizon_size, self.quantile_size,)

        return quantile_output, l1_penalties






